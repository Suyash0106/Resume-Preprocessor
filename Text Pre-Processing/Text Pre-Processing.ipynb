{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 1\n",
    "\n",
    "## Task 2: Text Pre-Processing\n",
    "#### Student Name:Suyash Sathe\n",
    "#### Student ID: 29279208\n",
    "\n",
    "Date: 02/09/2018\n",
    "\n",
    "Environment: Python 3.6.4 and Jupyter notebook\n",
    "Libraries used: \n",
    "* itertools (for flattening the list of lists)\n",
    "* nltk - natural language toolkit (tokenizer, stemmer, stopwords, collocations and probabilities)\n",
    "* re (for regular expression, included in Anaconda Python 3.6) \n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "This analysis extracts data from approximately 250 text documents that contains employee resumes. Data is extracted by reading the input files according to the given list of files. These files are read from the folder \"Resumes\" which contains all the files.\n",
    "\n",
    "The text files are read and each file is stored in a list called \"data\". This data is tokenized and it further goes through the following pre-processing:\n",
    "\n",
    "* Digits removal\n",
    "* Removal of tokens whose length < 3\n",
    "* Stop-words removal\n",
    "* Identify the bigrams\n",
    "* remove tokens with threshold > 98%\n",
    "* remove tokens with threshold < 2%\n",
    "* Stemming\n",
    "\n",
    "Text pre-processing is performed for producing a lexical vocabulary of the tokens and the associated sparse count vector matrix for each abstract. The pre-processing included tokenisation, lemmatisation and removal of stopwords. The initial tokenised vocabulary of the corpus was 14236 words, which was reduced to 3802 words after pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "import nltk\n",
    "import re\n",
    "from nltk.collocations import *\n",
    "from itertools import chain\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Extract the data\n",
    "\n",
    "Extract the data from the given list of files and store it in the list *\"data\"*.\n",
    "\n",
    "**Note:**\n",
    "1. The input files are present in the *\"Resumes* folder. This folder should be present in same the folder as this *.ipynb* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "files = [219, 723, 475, 334, 717,  27, 590, 405, 848, 611, 555, 344, 760, 185, 127, 813, 724, 560,\n",
    "         233, 547, 427, 156, 529, 801, 213, 332, 538, 291, 120, 278,  97, 736, 704,  91, 432, 339,\n",
    "         275, 174, 808, 238, 466, 623, 663, 285, 582, 863, 336, 554, 232, 431, 415, 686, 270, 710,\n",
    "         226, 382, 660, 627, 448, 604, 331, 549, 155, 661,  63,  91, 592, 370, 563,   9, 169, 576,\n",
    "         772, 399, 574, 485, 612, 655, 683, 471, 218, 266, 534, 312, 520, 203, 360, 568,  13, 501,\n",
    "         432,   3, 243, 533,  55, 498, 443, 715, 187, 828, 453, 133, 164, 515, 441, 781,  35, 577,\n",
    "         379, 616, 186, 567, 168, 371, 273, 236, 305, 132, 329,  77, 772, 126, 509, 773, 472, 811,\n",
    "         679, 424, 701, 480, 658, 120, 682, 469, 495, 569,  82, 842, 425,  75, 523, 336, 410, 790,\n",
    "         539, 411, 574, 695, 558, 614, 850, 860, 373, 716, 287, 122, 502,  30,  83, 259, 764, 429,\n",
    "         643, 390, 489, 121, 653, 647, 591,  35, 765, 266, 447, 145, 832, 598, 230, 609, 774, 589,\n",
    "         477,  94, 410 ,658, 738, 454, 274, 414, 383, 338, 813, 464, 133, 302, 709, 713, 698, 628,\n",
    "         853, 599, 168, 145, 444, 740, 177, 856, 655, 193, 261, 513, 704, 302, 486,  92,  54,\n",
    "         452, 693, 537, 472, 135, 713, 746, 333, 391, 394, 698, 765,   4, 141, 703, 692, 414, 858,\n",
    "         769,  62, 593, 634, 762,   3, 176, 632,  35, 465, 455,  69, 607, 258, 521, 783, 1]\n",
    "\n",
    "#Read all the resumes in data[]\n",
    "files = list(set(files))\n",
    "for x in files:\n",
    "    resume = open(\"Resumes/resume_(\"+str(x)+\").txt\", \"r\", encoding=\"utf-8\")\n",
    "    application = resume.read()\n",
    "    data.append(application)\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Text Pre-processing\n",
    "\n",
    "Each element of *data* is read and is segmented according to the sentence.  These sentences are further tokenized and stored in a separate list. Following operations are performed on the elements of this list:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.a  Sentence segmentation, Normalization and Tokenization\n",
    "\n",
    "Each file is segmented into sentences. The first word of each sentence is normalized. The sentence is then tokenized. The list *'all_tokens'* contains tokens from all the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Length of all_tokens: 148624\n"
     ]
    }
   ],
   "source": [
    "sentences_tokens = []\n",
    "for i in range(len(files)):\n",
    "    ############Sentence segmentation############\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = sent_detector.tokenize(data[i].strip())\n",
    "   \n",
    "    ############Normalization and Tokenization of sentences############\n",
    "    for sent in sentences:\n",
    "        tokenizer = RegexpTokenizer(r\"\\w+(?:[-.]\\w+)?\")\n",
    "        sent_tokens = tokenizer.tokenize(sent)\n",
    "        sent_tokens[0] = sent_tokens[0].lower()\n",
    "        sentences_tokens.append(sent_tokens)\n",
    "\n",
    "# List of all tokens\n",
    "all_tokens=list(chain.from_iterable(sentences_tokens))\n",
    "\n",
    "print(\"OK\")\n",
    "print(\"Length of all_tokens: \"+str(len(all_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.b  Remove the non-alphabetical tokens\n",
    "\n",
    "List *'all_tokens_alpha'* contains all the non-digit tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Length of all_tokens_alpha: 138588\n"
     ]
    }
   ],
   "source": [
    "# Remove the non-alphabetical tokens\n",
    "all_tokens_alpha = []\n",
    "for each in all_tokens:\n",
    "    if each.isalpha():\n",
    "        all_tokens_alpha.append(each)\n",
    "print(\"OK\")\n",
    "print(\"Length of all_tokens_alpha: \"+str(len(all_tokens_alpha)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.c  Removal of tokens whose length < 3\n",
    "\n",
    "Remove the tokens from the *all_tokens_alpha* list whose length <3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Length of all_tokens_len: 119370\n"
     ]
    }
   ],
   "source": [
    "all_tokens_len = []\n",
    "for w in all_tokens_alpha:\n",
    "    if len(w)>2:\n",
    "        all_tokens_len.append(w)\n",
    "print(\"OK\")\n",
    "print(\"Length of all_tokens_len: \"+str(len(all_tokens_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.d  Stop-words removal\n",
    "\n",
    "Read the stop-words from the file *\"stopwords_en.txt\"* into a list. Use this list to remove the stop-words from the *all_tokens_len* list\n",
    "\n",
    "**Note:**\n",
    "1. The file *\"stopwords_en.txt\"* should be present in same the folder as this *.ipynb* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Length of all_tokens_stop: 99203\n"
     ]
    }
   ],
   "source": [
    "############Stopwords############\n",
    "stopwords = open(\"stopwords_en.txt\",\"r\")\n",
    "stopwords = stopwords.readlines()\n",
    "stopwords_list=[]\n",
    "for each in stopwords:\n",
    "    stopwords_list.append(each.strip())\n",
    "stopwords_set = list(set(stopwords_list))\n",
    "\n",
    "all_tokens_stop = []\n",
    "for w in all_tokens_len:\n",
    "    if w not in stopwords_set:\n",
    "        all_tokens_stop.append(w)\n",
    "\n",
    "print(\"OK\")\n",
    "print(\"Length of all_tokens_stop: \"+str(len(all_tokens_stop)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.e  Bigrams\n",
    "\n",
    "Collect the first 200 meaningful bigrams and store it in the list *bigrams* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Length of bigrams: 200\n"
     ]
    }
   ],
   "source": [
    "############ Bigrams ############\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(all_tokens_stop)\n",
    "\n",
    "# Set frequency = 14 to get the top 200 bigrams\n",
    "bigram_finder.apply_freq_filter(14)\n",
    "\n",
    "# Top-200 bigrams\n",
    "top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200) \n",
    "\n",
    "bigrams = []\n",
    "for each in top_200_bigrams:\n",
    "    s = each[0]+\" \"+each[1]\n",
    "    bigrams.append(s)\n",
    "\n",
    "print(\"OK\")\n",
    "print(\"Length of bigrams: \"+str(len(bigrams)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.f  Removal of context-dependent and rare tokens\n",
    "\n",
    "Create a dictionary *context_dict* that contains the tokens and frequency of occurence of tokens in all the files. The tokens that occur in more than 98% files or less than 2% files are removed from the vocab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Length of all_tokens_stop_set: 4535\n"
     ]
    }
   ],
   "source": [
    "# Set of unique tokens\n",
    "all_tokens_stop_set = list(set(all_tokens_stop))\n",
    "\n",
    "# Dictionary to store the count of tokens\n",
    "context_dict = dict()\n",
    "\n",
    "# Initialize the count of each token to 0\n",
    "for each in all_tokens_stop_set:\n",
    "    context_dict[each] = 0\n",
    "\n",
    "# Calculate the frequency of tokens\n",
    "for token in all_tokens_stop_set:\n",
    "    for i in range(len(files)):\n",
    "        if token in data[i]:\n",
    "            context_dict[token]+=1\n",
    "\n",
    "# Remove the rare and context-dependent tokens \n",
    "for each in context_dict.keys():\n",
    "    if context_dict[each]>(0.98*225) or context_dict[each]<(0.02*225):\n",
    "        all_tokens_stop_set.remove(each)\n",
    "\n",
    "print(\"OK\")\n",
    "print(\"Length of all_tokens_stop_set: \"+str(len(all_tokens_stop_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.g  Stemming\n",
    "\n",
    "Porter stemmer is used for stemming the tokens.\n",
    "\n",
    "**Assumption:**<br>\n",
    "Only the tokens that are in lower case are sent to the stemmer since tokens that are in capital or tokens whose initial letter is capital are considered as whole words that occur between the sentences or are headings.\n",
    "\n",
    "If the tokens whose just initial letter is capital is sent to the stemmer, it converts the token to lower case which may not match to the original token in the text. Hence, not sent to the stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Length of all_tokens_stem: 3802\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "# Contains set of stemmed words\n",
    "all_tokens_stem = set()\n",
    "\n",
    "for word in all_tokens_stop_set:\n",
    "    # Stem only lower case tokens\n",
    "    if word.islower():\n",
    "        x = ps.stem(word)\n",
    "        all_tokens_stem.add(x)\n",
    "    else:\n",
    "        all_tokens_stem.add(word)\n",
    "all_tokens_stem= list(all_tokens_stem)\n",
    "\n",
    "# add the bigrams to the set of stemmen words\n",
    "for each in bigrams:\n",
    "    all_tokens_stem.append(each)\n",
    "\n",
    "# Sort the tokens\n",
    "all_tokens_stem=sorted(all_tokens_stem)\n",
    "\n",
    "print(\"OK\")\n",
    "print(\"Length of all_tokens_stem after adding bigrams: \"+str(len(all_tokens_stem)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Vocabulary\n",
    "\n",
    "It contains the bigrams and unigrams tokens in the following format, token_string:integer_index. Words in the vocabulary are sorted in alphabetical order and are stored in *\"29279208_vocab.txt\"* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "word_id = {}\n",
    "id=1\n",
    "for each in all_tokens_stem:\n",
    "    word_id[each] = id\n",
    "    id+=1\n",
    "\n",
    "vocab = open(\"29279208_vocab.txt\",\"w+\",encoding=\"utf-8\")\n",
    "for each in word_id:\n",
    "    vocab.write(each+\" : \"+str(word_id[each])+\"\\n\")\n",
    "    \n",
    "vocab.close()\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.  Context Vector\n",
    "\n",
    "The txt file contains all the “selected” resumes in the data-set. Each line in the txt file contains the sparse representations of one of the resumes in the data-set in the following format file_name, token_index:count, token_index:count,...\n",
    "\n",
    "The output is stored in the file *\"29279208_countVec.txt\"*\n",
    "\n",
    "**Note:**\n",
    "1. This code takes 2-3 minutes execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "countVec = open(\"29279208_countVec.txt\",\"w+\",encoding=\"utf-8\")\n",
    "\n",
    "for i in range(len(files)):\n",
    "    countVec.write(\"resume_(\"+str(files[i])+\"), \")\n",
    "    for each in word_id.keys():    \n",
    "        count_each = re.findall(each,data[i])\n",
    "        if len(count_each) >0:\n",
    "            countVec.write(str(word_id[each])+\" : \"+ str(len(count_each)) + \", \")\n",
    "    countVec.write(\"\\n\\n\")\n",
    "    \n",
    "countVec.close()\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.  Reference\n",
    "\n",
    "1. Tutorial materials from Week 4 to Week5.\n",
    "2. https://stackoverflow.com/\n",
    "3. https://youtube.com/\n",
    "4. https://www.tutorialspoint.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
